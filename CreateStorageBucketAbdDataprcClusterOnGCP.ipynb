{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provisioning Storage Bucket and Dataproc Cluster in the Cloud\n",
    "\n",
    "In this notebook you will have the opportunity to create a Storage Bucket and a Dataproc Cluster on Google Cloud.\n",
    "\n",
    "After enabling these services you may run jobs on Dataproc cluster using the code and the data files that are saved on your project Bucket.\n",
    "\n",
    "PS. When done, don't forget to delete the cluster so to avoid undesirable charges. The cost of storage is pretty low as you can see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up a Cloud Storage Bucket\n",
    "\n",
    "1. Go to the [project selector](https://console.cloud.google.com/projectselector2/home/dashboard) in Google Cloud and either select an existing or create a new project. \n",
    "\n",
    "2. Press the **Navigate menu**, scroll down, click on **Cloud Storage** --> **Buckets** and then press **Create**.\n",
    "<img src=\"images/bucket0_2.png\" alt=\"bucket\" width='100%'/>\n",
    "\n",
    "\n",
    "3. Fill in the **Name your bucket** field with an appropriate and unique name and press **Create**. \n",
    "<img src=\"images/bucket1_2.png\" alt=\"bucket\" width='100%'/>\n",
    "\n",
    "\n",
    "4. When done the bucket will be created and you will see the following where you have the options to CREATE FOLDER, UPLOAD FOLDER, UPLOAD FILES or to drag and drop files. You can also delete folders and files after picking them. In the following screenshot two folders are created.<br> In case you need the path to the created bucket (ie creating a cluster or moving files from the master node to your bucket) it is **gs://xxxxxxx** where xxxxxx is the name of your bucket.\n",
    "<img src=\"images/bucket2_2.png\" alt=\"bucket\" width='100%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up a Dataproc Cluster\n",
    "\n",
    "1. Go to the [project selector](https://console.cloud.google.com/projectselector2/home/dashboard) in Google Cloud and either select an existing or create a new project. Note, since Dataproc will provision VMs to run services, you are billed (of course) for the resources you reserve. Hence, it is suggested that you create a new project (e.g., i called mine osm-project) so that when finished you can immediately just delete the project and de-reserve any resources you have created.\n",
    "\n",
    "2. Click on the **Navigation menu** (on the left side of the bar), scroll down, find and click on Dataproc\n",
    "\n",
    "<img src=\"images/cluster00.png\" alt=\"clusters\" width='100%'/>\n",
    "\n",
    "or write in the search bar **Dataproc**.\n",
    "\n",
    "<img src=\"images/cluster01.png\" alt=\"clusters\" width='100%'/>\n",
    "\n",
    "\n",
    "2. If the project is new, or if you have never used Dataproc before, you need to enable the Dataproc API for your project.\n",
    "\n",
    "<img src=\"images/cluster02.png\" alt=\"clusters\" width='100%'/>\n",
    "\n",
    "3. Now, navigate to Dataproc by clicking [here](https://console.cloud.google.com/dataproc/clusters) and let's start by configuring our first Spark cluster.\n",
    "\n",
    "<img src=\"images/dataproc2.png\" alt=\"dataproc\" width='100%'/>\n",
    "\n",
    "4. Give your cluster a name (e.g., osm-cluster), select a region to deploy the worker nodes (e.g., eu-central2) and select for `standard` for cluster type. High-availability is the option you would use in production where you would like data replication and failure handling (of course, it is a bit more expensive).\n",
    "\n",
    "<img src=\"images/cluster04.png\" alt=\"clusters\" width='100%'/>\n",
    "\n",
    "Scroll down and choose any of the optional components you may need. Note that you will be billed for the use of these components in the way of pay as you go.\n",
    "\n",
    "<img src=\"images/cluster05.png\" alt=\"clusters\" width='100%'/>\n",
    "\n",
    "5. Pressing the tab **Configure nodes** you can see (and change) the main characteristcs of the cluster, ie the number of the workers, the cpu, the RAM, the capacity of the disks for the master and the workers.\n",
    "\n",
    "<img src=\"images/cluster06.png\" alt=\"clusters\" width='100%'/>\n",
    "\n",
    "6. In the tab Customize cluster you can browse and define the storage bucket you already created\n",
    "\n",
    "<img src=\"images/cluster07.png\" alt=\"clusters\" width='100%'/>\n",
    "\n",
    "7. When finished click **Create**. It will take 2-3min until the cluster is ready and by default, the standard cluster will create 2 VMs for the cluster but if the job submitted is intensive you may increase/decrease resources whenever you want!\n",
    "\n",
    "\n",
    "8. You have now successfully created a Dataproc cluster with just a couple of clicks!\n",
    "\n",
    "<img src=\"images/cluster08.png\" alt=\"clusters\" width='100%'/>\n",
    "\n",
    "9. Click on the **name of the cluster** and  then click the tab **VM INSTANCES**. You can see the master and the workers of the cluster and clicking on them, the details of each one will be presented.\n",
    "\n",
    "<img src=\"images/cluster09.png\" alt=\"clusters\" width='100%'/>\n",
    "\n",
    "10. Clicking on **SSH** option you can have access on the master node through SSH connection. You can use the CLI (command line interface) to execute Linux commands. As an example a file is downloaded with WGET and then this file is moved to our google storage bucket, that created previously, so to be used from your jupyter notebooks.\n",
    "\n",
    "\n",
    "<img src=\"images/cluster10.png\" alt=\"clusters\" width='100%'/>\n",
    "\n",
    "Using the CLI you work on your master VM. This is very useful but note that after deleting the cluster all the files on your master will be deleted too. If you need to handle the files on your Storage Bucket, especially with large files more than 2-3 GB,  a good practice is to create a directory (mkdir ~/myBucket) and mount your Storage Bucket (gcsfuse my-Storage-Bucket ~/mybucket). Typing cd ~/myBucket and ls you will see the file stored in your Storage Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
